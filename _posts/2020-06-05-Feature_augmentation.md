---
title:  特征衍生、特征筛选
categories: ML/DL
tags: 机器学习和建模
description: 
comments: true
author: Tang Qi
sidebar:
  nav: docs-cn
aside:
  toc: true
layout: post
---

特征优选，让模型自适应地选择最优特征，进而实现稀疏化，降低过拟合。包括两个方面：特征衍生和特征筛选。

<!--more-->

# <font face="黑体" color=green size=5>一、业务衍生</font>

1. **定义**：结合对业务理解，设计能够表征数据特点的新特征变量，利用业务思维和统计方法进行衍生，提取次级特征。

2. **方法**：逻辑关联、增量、频率分析、相对强度水平等：

   - 逻辑关联： 主要是从业务逻辑思路里提取新的可用特征；

    - 增量:  从变化幅度大的特征提取出其增量特征；
    - 频率分析:  从覆盖面较大且类别较多的分类特征取出其频率特征；
    -  相对强度水平:  从特征值分布有差异的数值型特征中提取其相对整体平均值的强度水平或相对某一群体平均值的强度水平。

3. **举例** (Recent, Frequency, Money, Time)：

    - 时间切片。以最近7天，最近30天，最近60天，最近半年，最近1年等，计算期间内发生的交易金额。在信用风险评分模型中， 一般以3+，30+，60+，90+，180+ 等时间长度来衍生变量，具体设计时结合业务场景考虑；
    - 金额的衍生。通常以最大金额、最小金额、平均金额、总金额来扩展，然后加上期间内的交易总次数；
    - 交易的类别。将相似的购物消费类别合并，可以进一步做业务分群。

# <font face="黑体" color=green size=5>二、技术衍生</font>

1. **定义**： 从特征之间关系和特点出发，选择相应衍生方法生产具有高预测性区分性的特征，这一过程也会产生大量无效或者低效的特征，这些特征在衍生阶段不做处理，会在后续的特征选择中被自动筛选。 

 **“显式” 运算和 “隐式” 运算**

2. **“显式” 运算（自动化工具：python 库 feature tools）**

    “显式”运算，即我们可以清楚地知道原始变量怎么一步步计算出新的变量的；特征扩展 （one-hot），包括：特征组合，特征扩展，合成特征。

   - 特征组合：指将两个或多个输入特征通过数学运算进行组合。在特征计算层次上对特征进行大范围加工衍生，比如相关特征的加减乘除方、二值化、离散化、交叉组合、多项式融合。
   - 数值运算 ：对特征进行加，减，乘，除，前提是这些数值特征具有一定的运算意义；
   - 二值化将细粒度的特征度量转化成粗粒度的度量，使得特征的差异化更大，是与某个阈值比较，把特征的取值转化为0或1，核心在于设定一个阈值。类似分类特征的独热编码处理。二值化的手段更适用于在某些值或某取值段分布差异大的（偏正态分布）或者缺失严重的情况，这一通过二值化，可以把数值特征的类别属性提取出来，也增强了特征的稳定性。比如取值为有理数的连续值特征（距离特征），有固定几个取值的枚举值特征（时间特征）。
   -  特征离散化，是对连续取值的特征进行转换衍生的方法，比如年龄等这样特征的取值相加相减是没有实际意义的的数值特征，连续特征离散化的基本假设是连续特征不同区间的取值对结果的贡献是不一样的。特征离散化就可以提取这些特征的非线性影响并减弱个别数值（或异常值）的作用强度。
   - 特征交叉 - 对多个特征进行交叉组合，或做交，并，补，笛卡尔集等运算。 暴力交叉，暴力交叉可能产生稀疏问题。合成特征：通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。是一种让线性模型学习到非线性特征的方式，包括以下类型：
     + 将一个特征与其本身或其他特征相乘（称为特征组合）。比如属性A有三个特征，属性B有两个特征，笛卡尔积后就有六个组合特征;
     + 两个特征相除;
     + 对连续特征进行分桶，以分为多个区间分箱。

   - 特征多项式融合，是可以获得特征的更高维度和互相间关系的项，多项式融合法不仅可以得到特征的交叉组合项还可以得到特征的高次项。在python中通过 ***\*PolynomialFeatures\****函数实现特征多项式生成和修整。（kernel核函数）

   - 特征扩展：基于一个特征，使用特征值打平（扩展）的方式衍生多个标注类型的特征，也可以理解为离散化。对于分类变量，直接one-hot编码；对于数值型特征，离散化到几个固定的区间段，然后用one-hot编码。

3. **“隐式”运算**，或算法根据信息增量构建和筛选特征 (决策树相关，比如GBDT或者随机森林）

   “隐式”运算，是指利用深度学习方法生成的一些非线性新特征，常被用于模型融合场景中，得到的新变量和原来的变量的关系难以用一个函数来表达出来。

   - 决策树系列算法

     在决策树的系列算法中，每个样本都会落入一个叶子结点上，将叶子结点作为新的特征用于训练模型:

     ​	使用GBDT来发掘有区分度的特征以及组合特征，然后直接用于逻辑回归模型中，常见的融合模型有"GBDT+LR"、"GBDT+FM"都是应用比较广泛。GBDT构建新特征的主要思想：GBDT每棵树的路径直接作为LR输入特征使用。用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。

   - 遗传算法

     遗传规划是一种启发式的因子演化技术，适合进行特征工程。传统的监督学习算法主要运用于特征与标签之间关系的拟合，而遗传规划则更多运用于特征挖掘(特征工程)，能从有限的量价数据中挖掘出具有增量信息的因子。 

![Genetic algorithm](https://github.com/iqgnat/iqgnat.github.io/raw/master/assets/images/2020-06-05-Feature_augmentation/wps1.jpg)



​					一开始，一组未经选择和进化的原始因子会被随机生成(第一代因子)，通过某种规则计算每个因子的适				应度，从中选出适合的个体作为下一代进化的父代。 这些被选择出来的父代通过多种方法进化，形成不				同的后代因子，然后循环进行下一轮进化。随着迭代次数的增长，因子不断繁殖、变异、进化，从而不				断逼近数据分布的真相。 



4. **与或非**

   

# <font face="黑体" color=green size=5>三、特征筛选</font>

1. LASSO回归（使用L1正则化）：拟合广义线性模型，同时进行变量少选和复杂度调整。

​       在建立模型之初，为了尽量减小因缺少重要自变量而出现的模型偏差，通常会选择尽可能多的自变量。然而，建模过程需要寻找对因变量最具有强解释力的自变量集合，也就是通过自变量选择(指标选择、字段选择)来提高模型的解释性和预测精度。指标选择在统计建模过程中是极其重要的问题。

　　LASSO以缩小变量集（降阶）为思想，是一种收缩估计方法。LASSO方法可以将变量的系数进行压缩并使某些回归系数变为0，进而达到变量选择的目的，可以广泛的应用于模型改进与选择。通过选择惩罚函数，借用Lasso思想和方法实现变量选择的目的。LASSO算法则是一种能够实现指标集合精简的估计方法。

![LASSO](https://github.com/iqgnat/iqgnat.github.io/raw/master/assets/images/2020-06-05-Feature_augmentation/wps2.png)

​		第一个 SUM（∑）里面是一个线性回归求损失平方，第二个SUM（∑）是线性回归中系数的服从条件，用来约束解的区域。此外，LASSO 回归的整体损失求极小的样子改成拉格朗日形式，公式2：

![LASSO_2](https://github.com/iqgnat/iqgnat.github.io/raw/master/assets/images/2020-06-05-Feature_augmentation/wps3.png)



​		从式子里可以看到回归系数使用的是L1正则化，λ是惩罚参数或者叫做调节参数。L1范数的好处是当惩罚参数充分大时可以把某些待估的回归系数精确地收缩到0。


2. 基于 information criteria 筛选：

   - 赤池信息量准则（Akaike information criteria） 
     	AIC 是衡量模型拟合优良性和模型复杂性的一种标准，在建立多元回归模型时，变量过多，且有不显著的变量时，可以使用AIC准则结合逐步回归进行变量筛选。

     ​	逐步回归分为三种，向前逐步回归、向后逐步回归、和逐步回归。向前逐步回归的特点是将自变量一个一个当如模型中，每当放入一个变量时，都利用相应的检验准则检验，当加入的变量不能使得模型变得更优良时，变量将会被剔除，如此不断迭代，直到没有适合的新变量加入为止。向后逐步回归的特点是，将所有变量都放入模型之后，一个一个的剔除变量，将某一变量拿出模型而使得模型更优良时，将会剔除此变量。如此反复迭代，直到没有合适的变量剔除为止。逐步回归则是结合了以上的向前和向后逐步回归的特点。 
     
     ![LASSO_2](https://github.com/iqgnat/iqgnat.github.io/raw/master/assets/images/2020-06-05-Feature_augmentation/wps4.png)
     
     其中e^(2k/T)为惩罚因子(penalty factor)。AIC指标是常用的利用趋势估计预测模型的指标之一。选择模型时，选择AIC最大的模型。
     
   - 贝叶斯信息准则（Bayesian information criteria）
   
     ​			BIC（Bayesian InformationCriterion）贝叶斯信息准则与AIC相似，用于模型选择，1978年由Schwarz提出。训练模型时，增加参数数量，也就是增加模型复杂度，会增大似然函数，但是也会导致过拟合现象，针对该问题，AIC和BIC均引入了与模型参数个数相关的惩罚项，BIC的惩罚项比AIC的大，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。
   
   ![BIC](https://github.com/iqgnat/iqgnat.github.io/raw/master/assets/images/2020-06-05-Feature_augmentation/wps5.jpg)
   
   ​			其中，k为模型参数个数，n为样本数量，L为似然函数。kln(n)惩罚项在维数过大且训练样本数据相对		较少的情况下，可以有效避免出现维度灾难现象。
   
   
   
   ​	- Mallows’s CP、R2 等



【本文主要基于以下网络资源整理。如有侵权部分，请随时联系。】

主要参考资料：

1. https://zhuanlan.zhihu.com/p/114852465

2. https://www.cnblogs.com/yyy-blog/p/10420175.html

3. http://www.uml.org.cn/ai/201811051.asp

4. https://blog.csdn.net/sb19931201/article/details/65445514

5. 华泰证券：基于遗传规划的选股因子挖掘

6. 蚂蚁金服核心技术：百亿特征实时推荐算法揭秘






